{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from typing import Callable, List, Tuple\n",
    "import pytorch_lightning as pl  \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, CData\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import logging\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "#kobert\n",
    "from kobert_transformers.utils import get_tokenizer\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW, BertConfig, BertModel, PreTrainedTokenizer\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available : 1\n",
      "GPU running\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")   #gpu 사용\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "print('GPUs Available :', torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU running')\n",
    "else:\n",
    "    print('GPU not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Contents Parsing]: 100%|██████████| 35/35 [00:18<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokens Length] 5,352,250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#모두의말뭉치 파일 내 json 파일 목록\n",
    "def parse_paths(folder):\n",
    "    for current, dirs, files in os.walk(folder):\n",
    "        return [os.path.join(current, file) for file in files if file.endswith(\".json\")]\n",
    "    \n",
    "#json 파일 내 기사의 문장에서 128글자까지만 추출\n",
    "def parse_contents(files):\n",
    "    result = []\n",
    "    for file in tqdm(files, desc=\"[Contents Parsing]\"):    #진행상황 확인용\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        for doc in contents[\"document\"]:\n",
    "            for paragraph in doc[\"paragraph\"]:\n",
    "                result.append(paragraph[\"form\"][:128])\n",
    "                \n",
    "    print(\"[Tokens Length] {0:,}\".format(len(result)))\n",
    "    return result\n",
    "\n",
    "data = parse_paths('C:\\BootCamp\\CP2\\Korpus')\n",
    "contents = parse_contents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4817025, 535225)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train : val = 9 : 1\n",
    "train, val = train_test_split(contents, test_size=0.1, random_state=111)\n",
    "\n",
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  3304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['차기대통령 첫 덕목은 ‘소통과 통합’',\n",
       " '‘청렴·도덕성’ ‘강력 리더십’ 順 반기문·문재인 2강 이재명 1중',\n",
       " '조기 대선이 가시화된 가운데 차기대통령이 갖춰야 할 덕목으로 국민 3명 중 1명은 ‘소통 및 사회통합 능력’을꼽았다.',\n",
       " ' 차기 대선후보 선호도에서는반기문 전 유엔사무총장(21.7%)과 문재인 전 더불어민주당 대표(18.5%)가오차범위 내 접전인 가운데 이재명 성남시장(11.5%)이 뒤를 쫓는 ‘2강 1중’구도로 나타났다.',\n",
       " '1일 서울신문이 새해를 맞아 에이스리서치에 의뢰해 지난달 28~29일 19세이상 남녀 1009명을 대상으로 벌인 여론조사(표본오차 95% 신뢰수준에서±3.1% 포인트)에 따르면 차기 대통령이 갖춰야 할 덕목으로 ‘소통 및 사회통합 능력’(34.3%), ‘청렴성 및 도덕성’(24.8%)이 우선 꼽혔다.',\n",
       " ' 이런 덕목은일방통행식 국정 운영과 최순실 국정 농단 등 박근혜 대통령의 탄핵 사유와 밀접한 관련이 있다는 점에서 차기 대선구도와 맞물려 시사하는 바가 크다.',\n",
       " '특히 올 경제성장률이 외환위기 이후 20년 만에 2%(정부 2.6%)로 전망되는 등 최악의 위기 상황임에도 ‘강력한 리더십’(13.4%)이나 ‘경제활성화능력’(12.5%)은 후순위였고 ‘정치 경험 및 경륜’(6.4%), ‘외교·안보·통일전문성’(4.5%)에 대한 갈증도 미미했다는 점은 주목할 만하다.',\n",
       " '2강 1중을 잇는 여야 차기 대선후보는 안철수 국민의당 전 공동대표(5.7%), 박원순 서울시장(3.0%), 손학규전 민주당 대표(2.1%) 순으로 나타났다.',\n",
       " ' 반 전 총장이 범여권 후보로 나서고민주당 문 전 대표와 국민의당 안 전 대표가 ‘가상 3자대결’을 벌인다면 반 전총장과 문 전 대표가 각각 31.1%와 30.4%로 0.7% 포인트 차이로 초박빙 양상으로 조사됐다.',\n",
       " ' 안 전 대표는 11.3%에 그쳤다.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 데이터셋\n",
    "def testset(test_data_path):\n",
    "    testset = []\n",
    "    for (path, dir, files) in os.walk(test_data_path):\n",
    "        for filename in files :\n",
    "            if filename[-6:] == '00.xml':   # 페이지 정보\n",
    "                continue\n",
    "            elif filename[-4:] == '.xml':\n",
    "                try : \n",
    "                    with open(\"%s/%s\" % (path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                        soup = BeautifulSoup(f, \"html.parser\")\n",
    "                        testset.append(soup.find(\"headline\").text)\n",
    "                        testset.append(soup.find(\"subheadline\").text)\n",
    "            \n",
    "                        t = soup.find(\"datacontent\").text\n",
    "                        t = t.split('다.')\n",
    "                        temp = []\n",
    "                        for tt in t[:-1] :\n",
    "                            temp.append(tt+'다.')\n",
    "                        temp.append(t[-1])\n",
    "                        testset.extend(temp)\n",
    "                        \n",
    "                        for i in testset:\n",
    "                            if i == '전면광고' or i == '전면광고\\n':\n",
    "                                testset.remove(i)\n",
    "\n",
    "                except:\n",
    "                    print('not exist ', \"%s/%s\" % (test_data_path, filename))  # 지면광고, 전면광고\n",
    "    return testset\n",
    "\n",
    "testset = testset('C:\\BootCamp\\CP2\\PDF')\n",
    "\n",
    "print(\"len: \", len(testset))\n",
    "testset[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "#### KoBERT모델의 입력 데이터 형태로 만들기\n",
    "    -CorpusDataset 클래스\n",
    "    -Preprocessor 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KoBertTokenizer\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "    \"vocab_txt\": \"vocab.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False},\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = \"▁\"\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    SentencePiece based tokenizer. Peculiarities:\n",
    "        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        vocab_txt,\n",
    "        do_lower_case=False,\n",
    "        remove_space=True,\n",
    "        keep_accents=False,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\n",
    "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                \"pip install sentencepiece\"\n",
    "            )\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize a string.\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        pieces = self.sp_model.encode(text, out_type=str)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A KoBERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(\n",
    "                map(\n",
    "                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n",
    "                    token_ids_0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A KoBERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "        to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, sentences, transform: Callable[[List, List], Tuple]):\n",
    "        self.sentences = []\n",
    "        self.slot_labels = [\"UNK\", \"PAD\", \"B\", \"I\"]\n",
    "        self.transform = transform\n",
    "        self._load_data(sentences)\n",
    "\n",
    "    def _load_data(self, sentences):\n",
    "        \"\"\"data를 file에서 불러온다.\n",
    "\n",
    "        Args:\n",
    "            data_path: file 경로\n",
    "        \"\"\"\n",
    "        self.sentences = [sen.split() for sen in sentences]\n",
    "        # with open(data_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #     lines = f.readlines()\n",
    "        #     self.sentences = [line.split() for line in lines]\n",
    "\n",
    "    def _get_tags(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"문장에 대해 띄어쓰기 tagging을 한다.\n",
    "        character 단위로 분리하여 BI tagging을 한다.\n",
    "\n",
    "        Args:\n",
    "            sentence: 문장\n",
    "\n",
    "        Retrns:\n",
    "            문장의 각 토큰에 대해 tagging한 결과 리턴\n",
    "            [\"B\", \"I\"]\n",
    "        \"\"\"\n",
    "\n",
    "        tags = []\n",
    "        for word in sentence:\n",
    "            for i in range(len(word)):\n",
    "                if i == 0:\n",
    "                    tags.append(\"B\")\n",
    "                else:\n",
    "                    tags.append(\"I\")\n",
    "        return tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = \"\".join(self.sentences[idx])\n",
    "        sentence = [s for s in sentence]\n",
    "        tags = self._get_tags(self.sentences[idx])\n",
    "        tags = [self.slot_labels.index(t) for t in tags]\n",
    "\n",
    "        (\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            slot_label_ids, \n",
    "        ) = self.transform(sentence, tags)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, slot_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, max_len: int):\n",
    "        self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "        self.max_len = max_len\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def get_input_features(\n",
    "        self, sentence: List[str], tags: List[str]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"문장과 띄어쓰기 tagging에 대해 feature로 변환한다.\n",
    "\n",
    "        Args:\n",
    "            sentence: 문장\n",
    "            tags: 띄어쓰기 tagging\n",
    "\n",
    "        Returns:\n",
    "            feature를 리턴한다.\n",
    "            input_ids, attention_mask, token_type_ids, slot_labels\n",
    "        \"\"\"\n",
    "\n",
    "        input_tokens = []\n",
    "        slot_label_ids = []\n",
    "\t\t\t\t\t\n",
    "        # tokenize\n",
    "        for word, tag in zip(sentence, tags):\n",
    "            tokens = self.tokenizer.tokenize(word)\n",
    "\n",
    "            if len(tokens) == 0:\n",
    "                tokens = self.tokenizer.unk_token\n",
    "\n",
    "            input_tokens.extend(tokens)\n",
    "\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    slot_label_ids.extend([tag])\n",
    "                else:\n",
    "                    slot_label_ids.extend([self.pad_token_id])\n",
    "\n",
    "        # max_len보다 길이가 길면 뒤에 자르기\n",
    "        if len(input_tokens) > self.max_len - 2:\n",
    "            input_tokens = input_tokens[: self.max_len - 2]\n",
    "            slot_label_ids = slot_label_ids[: self.max_len - 2]\n",
    "\n",
    "        # cls, sep 추가\n",
    "        input_tokens = (\n",
    "            [self.tokenizer.cls_token] + input_tokens + [self.tokenizer.sep_token]\n",
    "        )\n",
    "        slot_label_ids = [self.pad_token_id] + slot_label_ids + [self.pad_token_id]\n",
    "\n",
    "        # token을 id로 변환\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        # padding\n",
    "        pad_len = self.max_len - len(input_tokens)\n",
    "        input_ids = input_ids + ([self.tokenizer.pad_token_id] * pad_len)\n",
    "        slot_label_ids = slot_label_ids + ([self.pad_token_id] * pad_len)\n",
    "        attention_mask = attention_mask + ([0] * pad_len)\n",
    "        token_type_ids = token_type_ids + ([0] * pad_len)\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\n",
    "        slot_label_ids = torch.tensor(slot_label_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, slot_label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacingBertModel(pl.LightningModule):\n",
    "    def __init__(self, config, dataset):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "        self.slot_labels_type = [\"UNK\", \"PAD\", \"B\", \"I\"]\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "        self.bert_config = BertConfig.from_pretrained(\n",
    "            self.config.model, num_labels=len(self.slot_labels_type)\n",
    "        )\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            self.config.bert_model, config=self.bert_config\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.config.dropout_rate)\n",
    "        self.linear = nn.Linear(\n",
    "            self.bert_config.hidden_size, len(self.slot_labels_type)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        return self.linear(self.dropout(outputs[0]))\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        loss = self._calculate_loss(outputs, slot_label_ids) # slot_labels : labels\n",
    "\n",
    "        return {\"loss\": loss, \"log\": {\"train_loss\": loss}}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        val_loss = self._calculate_loss(outputs, slot_label_ids)\n",
    "        pred_slot_labels, gt_slot_labels = self._convert_ids_to_labels(\n",
    "            outputs, slot_label_ids\n",
    "        )\n",
    "\n",
    "        val_f1 = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
    "\n",
    "        return {\"val_loss\": val_loss, \"val_f1\": val_f1}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_log = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": torch.stack([x[\"val_f1\"] for x in outputs]).mean(),\n",
    "        }\n",
    "\n",
    "        return {\"val_loss\": val_loss, \"progress_bar\": tensorboard_log}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
    "\n",
    "        pred_slot_labels, gt_slot_labels = self._convert_ids_to_labels(\n",
    "            self(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              token_type_ids=token_type_ids,\n",
    "            ), slot_label_ids\n",
    "        )\n",
    "\n",
    "        test_f1 = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
    "\n",
    "        return {\"test_f1\": test_f1, }\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_f1 = torch.stack([x[\"test_f1\"] for x in outputs]).mean()\n",
    "\n",
    "        test_step_outputs = {\n",
    "            \"test_f1\": test_f1,\n",
    "        }\n",
    "\n",
    "        return test_step_outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.config.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"val\"], batch_size=self.config.eval_batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"test\"], batch_size=self.config.eval_batch_size)\n",
    "\n",
    "    def _calculate_loss(self, outputs, labels):\n",
    "        active_logits = outputs.view(-1, len(self.slot_labels_type))\n",
    "        active_labels = labels.view(-1)\n",
    "        loss = F.cross_entropy(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _f1_score(self, gt_slot_labels, pred_slot_labels):\n",
    "        return torch.tensor(\n",
    "            f1_score(gt_slot_labels, pred_slot_labels), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def _convert_ids_to_labels(self, outputs, slot_labels):\n",
    "        _, y_hat = torch.max(outputs, dim=2)\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        slot_label_ids = slot_labels.detach().cpu().numpy()\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(self.slot_labels_type)}\n",
    "        slot_gt_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
    "        slot_pred_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(slot_label_ids.shape[0]):\n",
    "            for j in range(slot_label_ids.shape[1]):\n",
    "                if slot_label_ids[i, j] != self.pad_token_id:\n",
    "                    slot_gt_labels[i].append(slot_label_map[slot_label_ids[i][j]])\n",
    "                    slot_pred_labels[i].append(slot_label_map[y_hat[i][j]])\n",
    "\n",
    "        return slot_pred_labels, slot_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'bert_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21976\\3879437859.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpusDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_input_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpacingBertModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m logger = TensorBoardLogger(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21976\\4280566711.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, dataset)\u001b[0m\n\u001b[0;32m     11\u001b[0m         )\n\u001b[0;32m     12\u001b[0m         self.model = BertModel.from_pretrained(\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         )\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'bert_model'"
     ]
    }
   ],
   "source": [
    "class Config():\n",
    "    def __init__(self) :\n",
    "        self.task= 'korean_spacing'\n",
    "        self.log_path= 'C:\\BootCamp\\CP2\\logs'\n",
    "        self.model =  'monologg/kobert'\n",
    "        self.train_data_path= 'C:\\BootCamp\\CP2\\train.txt'\n",
    "        self.val_data_path= 'C:\\BootCamp\\CP2\\val.txt'\n",
    "        self.test_data_path= 'C:\\BootCamp\\CP2\\test.txt'\n",
    "        self.max_len= 128\n",
    "        self.train_batch_size= 16\n",
    "        self.eval_batch_size= 16\n",
    "        self.dropout_rate= 0.1\n",
    "        self.gpus= torch.cuda.device_count()\n",
    "\n",
    "config = Config()\n",
    "\n",
    "preprocessor = Preprocessor(config.max_len)\n",
    "\n",
    "dataset = {}\n",
    "dataset[\"train\"] = CorpusDataset(train, preprocessor.get_input_features)\n",
    "dataset[\"val\"] = CorpusDataset(val, preprocessor.get_input_features)\n",
    "dataset[\"test\"] = CorpusDataset(testset, preprocessor.get_input_features)\n",
    "\n",
    "model = SpacingBertModel(config, dataset).cuda()\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=os.path.join(config.log_path, config.task), version=1, name=config.task\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filename=\"checkpoints/\"+ config.task + \"/{epoch}_{val_loss:35f}\",\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "lrmonitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=config.gpus,\n",
    "    callbacks=[checkpoint, lrmonitor],\n",
    "    logger=logger,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 27% | 10% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 27% |  4% |\n",
      "GPUs Available : 1\n"
     ]
    }
   ],
   "source": [
    "free_gpu_cache()   \n",
    "\n",
    "device = torch.device(\"cuda:0\")   #gpu 사용\n",
    "print('GPUs Available :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeonok\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:595: UserWarning: Checkpoint directory C:\\BootCamp\\CP2\\logs\\korean_spacing\\korean_spacing\\version_1\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\yeonok\\anaconda3\\envs\\kobert\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | model   | BertModel | 92.2 M\n",
      "1 | dropout | Dropout   | 0     \n",
      "2 | linear  | Linear    | 3.1 K \n",
      "--------------------------------------\n",
      "92.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.2 M    Total params\n",
      "368.760   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m         \u001b[1;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    811\u001b[0m         )\n\u001b[1;32m--> 812\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1405\u001b[0m             \u001b[1;31m# reload dataloaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             \u001b[0mval_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reload_evaluation_dataloaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m             self.num_sanity_val_batches = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py\u001b[0m in \u001b[0;36m_reload_evaluation_dataloaders\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dataloaders\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_reload_val_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_val_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mreset_val_dataloader\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             self.num_val_batches, self.val_dataloaders = self._data_connector._reset_eval_dataloader(\n\u001b[1;32m-> 1908\u001b[1;33m                 \u001b[0mRunningStage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVALIDATING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpl_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1909\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py\u001b[0m in \u001b[0;36m_reset_eval_dataloader\u001b[1;34m(self, mode, model)\u001b[0m\n\u001b[0;32m    416\u001b[0m                 orig_num_batches = num_batches = (\n\u001b[1;32m--> 417\u001b[1;33m                     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhas_len_all_ranks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py\u001b[0m in \u001b[0;36mhas_len_all_ranks\u001b[1;34m(dataloader, training_type, model)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mlocal_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mtotal_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13816\\2981585113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         self._call_and_handle_interrupt(\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         )\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconciliate_processes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_exception\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_teardown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             \u001b[1;31m# teardown might access the stage so we reset it after\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1298\u001b[0m         Callback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\n\u001b[0;32m   1299\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m         \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;31m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\strategies\\single_device.py\u001b[0m in \u001b[0;36mteardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# GPU teardown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;31m# clean up memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\core\\mixins\\device_dtype_mixin.py\u001b[0m in \u001b[0;36mcpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \"\"\"\n\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m         \"\"\"\n\u001b[1;32m--> 718\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    716\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m         \"\"\"\n\u001b[1;32m--> 718\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid argument"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeonok\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1447: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19176\\2096233821.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#테스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \"\"\"\n\u001b[0;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_and_handle_interrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     def _test_impl(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m         \u001b[1;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         self._ckpt_path = self.__set_ckpt_path(\n\u001b[1;32m--> 980\u001b[1;33m             \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m         )\n\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kobert\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__set_ckpt_path\u001b[1;34m(self, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[0;32m   1472\u001b[0m                     )\n\u001b[0;32m   1473\u001b[0m                 raise MisconfigurationException(\n\u001b[1;32m-> 1474\u001b[1;33m                     \u001b[1;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m                 )\n\u001b[0;32m   1476\u001b[0m             \u001b[1;31m# load best weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "#테스트\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델의 파라미터들을 저장 : torch.save(model.state_dict(), path)\n",
    "def save_checkpoint(epoch, model, optimizer, filename): \n",
    "    state = {'Epoch': epoch, \n",
    "             'State_dict': model.state_dict(), \n",
    "             'optimizer': optimizer.state_dict()} \n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 불러오기\n",
    "model = SpacingBertModel()   #모델 선언\n",
    "optimizer = optim.???    #옵티마이저 선언\n",
    "\n",
    "ckpt = torch.load(path)\n",
    "\n",
    "state_dict = model.load_state_dict(ckpt['State_dict'])\n",
    "opt = optimizer.load_state_dict(ckpt['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xml_path = 'C:\\BootCamp\\CP2\\PDF\\news_PDF'\n",
    "\n",
    "#xml내 텍스트 가져오기\n",
    "dic_count = {'head' : 0, 'sub' : 0, 'content':0, 'not exist' : 0, 'all' : 0, 'right' : 0}\n",
    "dic_article = {}\n",
    "\n",
    "for (path, dir, files) in os.walk(data_path):\n",
    "    for filename in files :\n",
    "        if filename[-6:] == '00.xml':   # 페이지 정보\n",
    "            with open(\"%s/%s\" % (path, filename), \"r\", encoding=\"EUC-KR\") as f:\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "                file_name = soup.find(\"file_name\").text\n",
    "                articles = soup.findAll(\"article\")\n",
    "                article_list = []\n",
    "                for article in articles :\n",
    "                    article_list.append(article.file_name.string)\n",
    "                    dic_article[file_name] = article_list\n",
    "    \n",
    "        elif filename[-4:] == '.xml':\n",
    "            try :\n",
    "                with open(\"%s/%s\" % (path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                b = True\n",
    "                dic_count['all'] += 1\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "                headline = soup.find(\"headline\")\n",
    "                subheadline = soup.find(\"subheadline\")\n",
    "                content = soup.find(\"datacontent\")\n",
    "                \n",
    "#텍스트를 input 형태로\n",
    "preprocessor = Preprocessor(config.max_len)\n",
    "dataset = {}\n",
    "dataset['test'] = CorpusDataset(content, preprocessor.get_input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = '''조기 대선이 가시화된 가운데 차기\n",
    "대통령이 갖춰야할덕목으로 국민 3\n",
    "명 중 1명은 ‘소통 및사회통합 능력’을\n",
    "꼽았다. 차기 대선후보 선호도에서는\n",
    "반기문 전 유엔사무총장(21.7%)과 문\n",
    "재인 전 더불어민주당 대표(18.5%)가\n",
    "오차범위 내 접전인 가운데 이재명 성\n",
    "남시장(11.5%)이 뒤를 쫓는 ‘2강 1중’\n",
    "구도로나타났다.\n",
    "1일 서울신문이 새해를 맞아 에이스\n",
    "리서치에 의뢰해 지난달 28~29일 19세\n",
    "이상 남녀 1009명을 대상으로 벌인 여\n",
    "론조사(표본오차 95% 신뢰수준에서\n",
    "±3.1% 포인트)에 따르면 차기 대통령\n",
    "이 갖춰야 할 덕목으로 ‘소통 및사회통\n",
    "합 능력’(34.3%), ‘청렴성 및도덕성’\n",
    "(24.8%)이 우선 꼽혔다. 이런 덕목은\n",
    "일방통행식국정운영과최순실국정농\n",
    "단 등 박근혜 대통령의 탄핵 사유와 밀\n",
    "접한 관련이 있다는 점에서 차기 대선\n",
    "구도와맞물려시사하는바가크다.\n",
    "특히 올 경제성장률이 외환위기 이\n",
    "후 20년 만에 2%(정부 2.6%)로 전망\n",
    "되는 등 최악의 위기 상황임에도 ‘강력\n",
    "한 리더십’(13.4%)이나 ‘경제활성화\n",
    "능력’(12.5%)은 후순위였고 ‘정치 경\n",
    "험 및경륜’(6.4%), ‘외교·안보·통일\n",
    "전문성’(4.5%)에 대한 갈증도 미미했\n",
    "다는 점은주목할만하다.\n",
    "2강 1중을 잇는 여야 차기 대선후보\n",
    "는 안철수 국민의당 전 공동대표(5.7\n",
    "%), 박원순 서울시장(3.0%), 손학규\n",
    "전 민주당 대표(2.1%) 순으로 나타났\n",
    "다. 반 전 총장이 범여권 후보로 나서고\n",
    "민주당 문 전 대표와 국민의당 안전대\n",
    "표가 ‘가상 3자대결’을 벌인다면 반 전\n",
    "총장과 문 전 대표가 각각 31.1%와 30.\n",
    "4%로 0.7% 포인트 차이로 초박빙 양\n",
    "상으로 조사됐다. 안 전 대표는 11.3%\n",
    "에 그쳤다.\n",
    "국회 개헌특위가 본격 가동되는 등\n",
    "정치권의 화두로 떠오른 대통령 임기\n",
    "축소를 중심으로 한 개헌 방안에 대해\n",
    "서는 찬성(44.5%)이 반대(38.7%)보\n",
    "다 5.8% 포인트 높았지만, 여전히 ‘모\n",
    "름·무응답’도 16.8%에 이르는 것으로\n",
    "나타났다.'''.replace(' ','').replace('\\n','')\n",
    "\n",
    "trainer.predict(???, return_predictions=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "954a413ef82e6f97dfd0364f6abb6276b9f32643231064a48e135ce381da3df2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
